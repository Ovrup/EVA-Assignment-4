{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FourthCodeAssignment4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ovrup/EVA-Assignment-4/blob/master/FourthCodeAssignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJAs0J9k8-9W",
        "colab_type": "text"
      },
      "source": [
        "**4th Code Assignment 4**\n",
        "\n",
        "This is same architecture from 3rd code but dropout is added. The no of parameters are also same as previous one. I used SGD here instead of Adam & learning rate scheduler also discarded. Batch size is 128. After 20 epoch achieved final validation accuracy 99.4%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir7AeWMDhuSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSjrr3Izh3pC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing all necessary modules in Keras\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRRChybch31I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mnist.load_data() function download the MNIST data set(Images of hand written digits) & divide it into training set & test set.\n",
        "# X_train holds the training samples & y_train holds the corresponding labels for training data. Likewise, X_test holds test data & y_test has the labels.\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjptflFrh37_",
        "colab_type": "code",
        "outputId": "33888c7f-4ecc-4745-f331-2cc220e218d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "# X_train is a matrix which has 60K images each of size 28*28. X_train.shape returns the shape of the total training set\n",
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "# visulaization of the 1st sample of training set which is an image of 5 as shown below.\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f53d3313f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1YYHLkMh4C1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The train & test set are 3D matrix.The training set is of shape (60000,28,28) & test set is (10000,28,28). But here the CNN model expects\n",
        "# the input shape to be 4D i.e (no of samples,height,width,channel).Channel implies whether the image is gray scale or colour(RGB). In case of\n",
        "# RGB image the 4th dimension would be 3. In this case the images are gray scale. Hence, it is 1. That's why training & test set are reshaped into\n",
        "# matrices of shape (60000,28,28,1) & (10000,28,28,1) respectively.\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HXK38mDh4JJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The data sets are matrices which has values between 0-255. These are normally the pixel values. It is a good practice to normalise the pixels.\n",
        "#  Hence, dividing by 255 scales down each pixel values between 0-1. But before doing that the data set values has to be converted to float to make sure\n",
        "# after dividing by 255 these are stored as float.\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz_GaFwXh4K8",
        "colab_type": "code",
        "outputId": "174dbe43-b43c-4506-8892-a78eb54241c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 1st 10 labels for training set. As printed below the 1st element is 5 which implies the 1st image of training set is of digit 5 and so on. \n",
        "y_train[:10]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4HAeMk-h4Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "# As shown above the label matrix(Y_train & Y_test) holds values that implies what digit the image is about. So, keras may find some relational order\n",
        "# between these values. To avoid this problem we hot encode the matrix into a binary matrix. This matrix has number of columns equal to the number\n",
        "# of classes(10 columns in this scenario). Each row defines the label of one sample point in data set & has only one '1' & others are '0'. '1' at\n",
        "# particular index position implies the digit equal to the column number. For example, the 1st row(1st sample of training data set) has '1' at column\n",
        "# number 5(starting from 0) which means this is image of digit 5.\n",
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices \n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnEjyfkyh4RJ",
        "colab_type": "code",
        "outputId": "a932db1f-070d-49a2-f701-4af2718601ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Label matrix hot encoded into binary matrix\n",
        "Y_train[:10]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy_ibvgrX4Ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Activation\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input = 28*28*1\n",
        "model.add(Convolution2D(10, kernel_size = (3,3), activation = 'relu', input_shape = (28, 28, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.15))\n",
        "\n",
        "# Input = 26*26*10\n",
        "model.add(Convolution2D(16, kernel_size = (3,3), activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "# Input = 24*24*16\n",
        "model.add(Convolution2D(16, kernel_size = (3,3), activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.15))\n",
        "\n",
        "# Input = 22*22*16\n",
        "model.add(Convolution2D(10, kernel_size=(1,1), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "# Input = 22*22*10\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "# Input = 11*11*10\n",
        "model.add(Convolution2D(16, kernel_size=(3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "# Input = 9*9*16\n",
        "model.add(Convolution2D(16, kernel_size=(3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.15))\n",
        "\n",
        "# Input = 7*7*16\n",
        "model.add(Convolution2D(10, kernel_size=(1,1), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dropout(0.15))\n",
        "\n",
        "# Input = 7*7*10\n",
        "model.add(Convolution2D(10, 7))\n",
        "\n",
        "# Input = 1*1*10\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRGYZKoyiUct",
        "colab_type": "code",
        "outputId": "7e5b8c5f-017b-4024-dbb1-40b97ca030bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "# summary() function displays the model structure i.e for each layer how many parameters used, what is the shape of output image\n",
        "model.summary()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_95 (Conv2D)           (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "batch_normalization_83 (Batc (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_96 (Conv2D)           (None, 24, 24, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_84 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_97 (Conv2D)           (None, 22, 22, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_85 (Batc (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_98 (Conv2D)           (None, 22, 22, 10)        170       \n",
            "_________________________________________________________________\n",
            "batch_normalization_86 (Batc (None, 22, 22, 10)        40        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 11, 11, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_99 (Conv2D)           (None, 9, 9, 16)          1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_87 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_100 (Conv2D)          (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_88 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_101 (Conv2D)          (None, 7, 7, 10)          170       \n",
            "_________________________________________________________________\n",
            "batch_normalization_89 (Batc (None, 7, 7, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_50 (Dropout)         (None, 7, 7, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_102 (Conv2D)          (None, 1, 1, 10)          4910      \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 13,278\n",
            "Trainable params: 13,090\n",
            "Non-trainable params: 188\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WivZHFD6qz8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKkc6pAmiUmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model with SGD optimizer. Since this is multi class problem (10 classes) 'categorical_crossentropy' is used as loss function.\n",
        "# accuracy is used as metrics which means while training this accuracy metrics will be monitored.\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=SGD(lr=0.01,decay = 0.00001, momentum = 0.8),\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZcCcCL3iUte",
        "colab_type": "code",
        "outputId": "e5170748-e60e-479a-9885-84dff0dea7f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "# Train the model with fit() which takes the training data set along with the label matrix. Epoch means one round of the whole data set. Batch size\n",
        "# determines in an epoch how many images to be processed parallelly. Batch size 32 means in an epoch in one iteration 32 images are processed.\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20,validation_data = (X_test,Y_test))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 10s 167us/step - loss: 0.0073 - acc: 0.9974 - val_loss: 0.0226 - val_acc: 0.9939\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0070 - acc: 0.9975 - val_loss: 0.0222 - val_acc: 0.9941\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0060 - acc: 0.9980 - val_loss: 0.0222 - val_acc: 0.9939\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0056 - acc: 0.9980 - val_loss: 0.0222 - val_acc: 0.9938\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0222 - val_acc: 0.9940\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0056 - acc: 0.9982 - val_loss: 0.0224 - val_acc: 0.9936\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0057 - acc: 0.9980 - val_loss: 0.0220 - val_acc: 0.9939\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0223 - val_acc: 0.9940\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0055 - acc: 0.9981 - val_loss: 0.0221 - val_acc: 0.9937\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0222 - val_acc: 0.9940\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0055 - acc: 0.9981 - val_loss: 0.0226 - val_acc: 0.9942\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0063 - acc: 0.9978 - val_loss: 0.0220 - val_acc: 0.9941\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0219 - val_acc: 0.9938\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.0219 - val_acc: 0.9944\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0218 - val_acc: 0.9939\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.0228 - val_acc: 0.9943\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0222 - val_acc: 0.9938\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0052 - acc: 0.9983 - val_loss: 0.0223 - val_acc: 0.9939\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0227 - val_acc: 0.9940\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0048 - acc: 0.9983 - val_loss: 0.0225 - val_acc: 0.9940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f53cd9f40b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUTlV4ooiU1D",
        "colab_type": "code",
        "outputId": "fdc23b99-d5d3-4b4f-d782-74c962cb3705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Now the model is trained which means the weights are optimized. model.evaluate() predicts the classes of each image in test set & then calculate\n",
        "# test set loss & accuracy.\n",
        "score = model.evaluate(X_test, Y_test, verbose=1)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 94us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAY1beNNiiXH",
        "colab_type": "code",
        "outputId": "d7de6e33-4492-4851-cf76-bc437be44765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# score is a vector that holds the test set loss & accuracy\n",
        "print(score)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.022523940222770853, 0.994]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnHdifAZiieX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict the class of each image in test set & stores in y_pred matrix\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5tTsReFiik-",
        "colab_type": "code",
        "outputId": "8ab39af5-e3aa-4351-bf73-72273f7e251f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.1839149e-15 1.1409783e-14 7.5047656e-13 6.9255528e-13 2.5250862e-19\n",
            "  9.0720928e-17 1.7745183e-21 1.0000000e+00 2.1962286e-18 5.3771573e-14]\n",
            " [3.7761627e-08 1.4465726e-09 1.0000000e+00 3.3211725e-14 5.1054918e-13\n",
            "  1.8442158e-20 2.1938123e-11 3.1135874e-13 7.8683737e-14 7.4114985e-15]\n",
            " [4.7409854e-09 1.0000000e+00 9.3281936e-12 1.3643670e-10 4.4028950e-10\n",
            "  2.7875084e-08 7.3854434e-09 2.4037741e-09 1.3961973e-10 3.4630323e-13]\n",
            " [9.9999809e-01 5.4025313e-17 3.1478360e-15 1.4516810e-12 2.3180788e-14\n",
            "  2.1091951e-13 1.8751088e-06 2.0172034e-14 7.5731366e-10 4.5210483e-10]\n",
            " [1.1487152e-17 6.0208934e-13 2.6231463e-15 5.6773224e-17 1.0000000e+00\n",
            "  5.3772110e-17 3.8185943e-14 7.6764261e-16 2.0379924e-12 2.5218452e-09]\n",
            " [8.4744117e-10 1.0000000e+00 1.7658973e-11 1.0636979e-13 1.2174946e-10\n",
            "  2.6144656e-11 1.6032994e-10 7.0318085e-10 8.6484882e-12 1.4887805e-14]\n",
            " [4.1276260e-21 1.0018353e-10 1.4434097e-15 7.4035146e-19 1.0000000e+00\n",
            "  1.0360305e-15 9.3474362e-18 5.2525036e-11 5.6746951e-12 8.6603652e-10]\n",
            " [2.7169158e-13 5.1886487e-14 1.3006443e-10 2.0959841e-10 9.1934226e-08\n",
            "  1.3125524e-13 4.5131308e-17 2.5588478e-13 1.1419318e-10 9.9999988e-01]\n",
            " [2.8788048e-07 2.7293440e-14 3.7016598e-10 7.9291651e-10 2.3816912e-12\n",
            "  9.8494351e-01 1.4926964e-02 3.4850480e-15 1.2926986e-04 7.6847595e-09]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxd3j3uKmtUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShGbYmvH7l4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_14'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 1, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}